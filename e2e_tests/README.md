# E2E Tests

End-to-end tests for rust-copartner suggestion system.

## How it works

Tests are organized by mode first, then by scenario:

```text
e2e_tests/
├── interactive/
│   └── scene1/
│       ├── original/     # Starting project
│       ├── edited/       # User's partial changes
│       ├── expect/       # Expected final result
│       └── actual/       # Generated during test
└── prompt/
    └── scene1/
        ├── original/     # Starting project
        ├── prompt.md     # Natural language description
        ├── expect/       # Expected final result
        └── actual/       # Generated during test
```

### Directory Structure

**Interactive mode** (`interactive/sceneX/`):

- `original/` - The starting project before any modifications
- `edited/` - Files the user actually modified (partial/incomplete changes)
- `expect/` - What we expect the final result to look like
- `actual/` - Generated by test script, working directory for daemon

**Prompt mode** (`prompt/sceneX/`):

- `original/` - The starting project before any modifications  
- `prompt.md` - Natural language description of desired changes
- `expect/` - What we expect the final result to look like
- `actual/` - Generated by test script, working directory for daemon

## Test Flow

1. **Setup**: Copy `original/` → `actual/`
2. **Generate diff**: Test script automatically creates diff between `original/` and `edited/`
3. **Process**: Feed the generated diff to rust-copartner daemon using `actual/` as project directory
4. **Apply**: Daemon suggests changes and applies them to `actual/`
5. **Verify**: Compare final `actual/` with `expect/` to see if those are the same

## Current Scenarios

- **scene1**: Simple struct rename (`Point` → `Point3D`) with field and implementation updates
- More scenarios coming soon...

## Running Tests

```bash
./run_e2e_tests.sh
```

The script handles daemon lifecycle, runs all scenarios, and reports which ones passed or failed.

## Adding New Scenarios

1. Create new `sceneX/` directory
2. Set up `original/` with your starting project
3. Create `edited/` with user's incomplete changes  
4. Design `expect/` with the ideal completion

The test script will automatically generate the diff and handle the rest. Keep scenarios focused on specific LLM capabilities you want to test (auto-completion, refactoring, import resolution, etc.).
