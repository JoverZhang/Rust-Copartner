# E2E Tests

End-to-end tests for rust-copartner suggestion system.

## How it works

Each test scenario lives in its own directory (like `interactive/scene1/`, `prompt/scene1/`, etc.). These tests cover both **Interactive** and **Prompt** modes. Here's what each subdirectory does:

### `interactive/`

Contains test data for **Interactive mode**:

- `edited/` - Files the user actually modified, representing partial/incomplete changes

### `prompt/`

Contains test data for **Prompt mode**:

- `prompt.md` - Natural language description of the desired changes

### Following directories are common to both modes

- `original/` - The starting point - a clean, working project before any user modifications. This is what the user's project looked like initially.
- `expect/` - The gold standard - what we expect the final result to look like after the LLM processes the changes. This is our target for comparison.
- `actual/` (automatically generated by the test script) - Working directory for the daemon during testing. Gets copied from `original/` at test start, then receives suggestions and modifications from the LLM. This is where all the magic happens.

## Test Flow

1. **Setup**: Copy `original/` → `actual/`
2. **Generate diff**: Test script automatically creates diff between `original/` and `edited/`
3. **Process**: Feed the generated diff to rust-copartner daemon using `actual/` as project directory
4. **Apply**: Daemon suggests changes and applies them to `actual/`
5. **Verify**: Compare final `actual/` with `expect/` to see if those are the same

## Current Scenarios

- **scene1**: Simple struct rename (`Point` → `Point3D`) with field and implementation updates
- More scenarios coming soon...

## Running Tests

```bash
./run_e2e_tests.sh
```

The script handles daemon lifecycle, runs all scenarios, and reports which ones passed or failed.

## Adding New Scenarios

1. Create new `sceneX/` directory
2. Set up `original/` with your starting project
3. Create `edited/` with user's incomplete changes  
4. Design `expect/` with the ideal completion

The test script will automatically generate the diff and handle the rest. Keep scenarios focused on specific LLM capabilities you want to test (auto-completion, refactoring, import resolution, etc.).
